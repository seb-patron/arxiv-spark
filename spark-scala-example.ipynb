{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97c869e6-bcfa-4d7b-8245-0043e7cb36d7",
   "metadata": {},
   "source": [
    "# Spark with Scala Example Notebook\n",
    "\n",
    "This notebook demonstrates how to use Apache Spark with Scala in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1abb04-62c1-475e-b733-3e8e68481aa6",
   "metadata": {},
   "source": [
    "## Checking Versions\n",
    "\n",
    "First, let's check which versions of Scala and Spark we're using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4117e9d4-685e-4bd9-a125-a7fc446c0bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scala version: version 2.12.18\n",
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "// Print Scala and Spark versions\n",
    "println(s\"Scala version: ${scala.util.Properties.versionString}\")\n",
    "println(s\"Spark version: ${spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2829b4a4-f3e7-478c-a718-17c83c1033a7",
   "metadata": {},
   "source": [
    "## Creating a DataFrame\n",
    "\n",
    "Let's create a simple DataFrame with some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a28e3a6-8cef-4627-b71e-aef36a43d221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1| John| 25|\n",
      "|  2|Alice| 30|\n",
      "|  3|  Bob| 35|\n",
      "|  4|Sarah| 28|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = List((1,John,25), (2,Alice,30), (3,Bob,35), (4,Sarah,28))\n",
       "df = [id: int, name: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 1 more field]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a simple DataFrame\n",
    "val data = Seq(\n",
    "  (1, \"John\", 25),\n",
    "  (2, \"Alice\", 30),\n",
    "  (3, \"Bob\", 35),\n",
    "  (4, \"Sarah\", 28)\n",
    ")\n",
    "\n",
    "val df = spark.createDataFrame(data).toDF(\"id\", \"name\", \"age\")\n",
    "\n",
    "// Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dea9f6-aa16-45d1-82b5-efc7a805a3b0",
   "metadata": {},
   "source": [
    "## Transforming Data\n",
    "\n",
    "Now let's perform some transformations on our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06bd03ca-44ad-4aac-8c09-569022359b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  2|Alice| 30|\n",
      "|  3|  Bob| 35|\n",
      "|  4|Sarah| 28|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "olderThan25 = [id: int, name: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, name: string ... 1 more field]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Filter for people older than 25\n",
    "val olderThan25 = df.filter($\"age\" > 25)\n",
    "olderThan25.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714fcf8-d409-4c3a-8b8c-8333290c6a10",
   "metadata": {},
   "source": [
    "## Working with RDDs\n",
    "\n",
    "While DataFrames are the modern API, you can also work with RDDs (Resilient Distributed Datasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a26e46c-d00b-4b24-8fbb-56edb03746e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of doubled even numbers from 1 to 100: 5100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd = ParallelCollectionRDD[0] at parallelize at <console>:25\n",
       "result = 5100\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "5100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a simple RDD\n",
    "val rdd = sc.parallelize(1 to 100)\n",
    "\n",
    "// Perform some transformations\n",
    "val result = rdd.filter(_ % 2 == 0).map(_ * 2).reduce(_ + _)\n",
    "println(s\"Sum of doubled even numbers from 1 to 100: $result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492527fb-2655-4df4-afe7-0d96e11c3630",
   "metadata": {},
   "source": [
    "## More Complex Example: Word Count\n",
    "\n",
    "Let's implement the classic word count example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5fa1bbe-a79c-49d7-b170-081e9349edc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark,4)\n",
      "(for,3)\n",
      "(the,3)\n",
      "(data,3)\n",
      "(an,3)\n",
      "(interface,2)\n",
      "(entire,2)\n",
      "(clusters,2)\n",
      "(programming,2)\n",
      "(provides,2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text = \n",
       "textRDD = ParallelCollectionRDD[3] at parallelize at <console>:31\n",
       "wordCounts = MapPartitionsRDD[12] at sortBy at <console>:39\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Apache Spark is an open-source unified analytics engine for large-scale data processing.\n",
       "Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
       "Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation,\n",
       "which has maintained it since. Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[12] at sortBy at <console>:39"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Sample text\n",
    "val text = \"\"\"Apache Spark is an open-source unified analytics engine for large-scale data processing.\n",
    "Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
    "Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since. Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.\"\"\"\n",
    "\n",
    "// Create an RDD from the text\n",
    "val textRDD = sc.parallelize(text.split(\"\\\\n\"))\n",
    "\n",
    "// Split into words, convert to lowercase, remove punctuation, and count\n",
    "val wordCounts = textRDD\n",
    "  .flatMap(line => line.toLowerCase.replaceAll(\"[^a-zA-Z ]\", \"\").split(\" \"))\n",
    "  .filter(word => word.length > 0)\n",
    "  .map(word => (word, 1))\n",
    "  .reduceByKey(_ + _)\n",
    "  .sortBy(_._2, ascending = false)\n",
    "\n",
    "// Show the top 10 most frequent words\n",
    "wordCounts.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9fdc3a-c771-4c40-8f0d-38e3ef6884b1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored:\n",
    "\n",
    "1. Creating and manipulating DataFrames\n",
    "2. Using SQL queries with Spark\n",
    "3. Working with RDDs\n",
    "4. Implementing a word count algorithm\n",
    "\n",
    "These are fundamental operations in Spark that form the building blocks for more complex data processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d381c5-f1c1-4456-bf11-a28fa11d6c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
