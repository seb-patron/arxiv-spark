{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66cfb577-bf2b-4ace-bb54-417de9aa47b5",
   "metadata": {},
   "source": [
    "arXiv kaggle dataset exploratation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c27afc24-bddd-47fe-947e-aa47b38f2fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Arxiv-Exploration\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.elasticsearch:elasticsearch-spark-30_2.12:8.8.2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3447cd44-fc6f-49e4-94cc-1eddc85447b8",
   "metadata": {},
   "source": [
    "import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2b4e59-aa28-4dc9-8f7a-97f46e63a29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      "\n",
      "Total records: 2694879\n",
      "+---------+--------------------+--------------------+--------------------+---------------+\n",
      "|       id|               title|            abstract|             authors|     categories|\n",
      "+---------+--------------------+--------------------+--------------------+---------------+\n",
      "|0704.0001|Calculation of pr...|  A fully differe...|C. Bal\\'azs, E. L...|         hep-ph|\n",
      "|0704.0002|Sparsity-certifyi...|  We describe a n...|Ileana Streinu an...|  math.CO cs.CG|\n",
      "|0704.0003|The evolution of ...|  The evolution o...|         Hongjun Pan| physics.gen-ph|\n",
      "|0704.0004|A determinant of ...|  We show that a ...|        David Callan|        math.CO|\n",
      "|0704.0005|From dyadic $\\Lam...|  In this paper w...|Wael Abu-Shammala...|math.CA math.FA|\n",
      "+---------+--------------------+--------------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the JSON file (adjust if necessary)\n",
    "data_path = \"../../data/arxiv-metadata-oai-snapshot.json\"\n",
    "\n",
    "# Load the JSON file into a DataFrame, selecting only the fields of interest\n",
    "df = spark.read.json(data_path) \\\n",
    "        .select(\"id\", \"title\", \"abstract\", \"authors\", \"categories\")\n",
    "\n",
    "# Print the schema and number of records to verify\n",
    "df.printSchema()\n",
    "print(\"Total records:\", df.count())\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4ec9f8-47c8-4e0f-9061-1af2a2c380e4",
   "metadata": {},
   "source": [
    "Now let's clean and normalize the fields:\n",
    "\n",
    "Remove any newline characters or excessive whitespace in titles and abstracts.\n",
    "\n",
    "(Optionally) convert text to lowercase for consistent processing.\n",
    "\n",
    "Clean the authors field (e.g., remove line breaks, unify separators).\n",
    "\n",
    "Split the categories field into an array of individual category codes, since categories in the raw data might be a single string of space-separated codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0998c86e-668c-4378-99b9-d0beb84dc7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n",
      "Total records: 2694879\n",
      "+---------+--------------------+--------------------+--------------------+------------------+\n",
      "|       id|               title|            abstract|             authors|        categories|\n",
      "+---------+--------------------+--------------------+--------------------+------------------+\n",
      "|0704.0001|calculation of pr...|a fully different...|c. bal\\'azs, e. l...|          [hep-ph]|\n",
      "|0704.0002|sparsity-certifyi...|we describe a new...|ileana streinu, l...|  [math.CO, cs.CG]|\n",
      "|0704.0003|the evolution of ...|the evolution of ...|         hongjun pan|  [physics.gen-ph]|\n",
      "|0704.0004|a determinant of ...|we show that a de...|        david callan|         [math.CO]|\n",
      "|0704.0005|from dyadic $\\lam...|in this paper we ...|wael abu-shammala...|[math.CA, math.FA]|\n",
      "+---------+--------------------+--------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, trim, lower, split\n",
    "\n",
    "# Remove newlines and excessive whitespace in title and abstract, and trim\n",
    "df_clean = df.withColumn(\"title\", trim(regexp_replace(\"title\", r\"[\\r\\n]+\", \" \"))) \\\n",
    "             .withColumn(\"abstract\", trim(regexp_replace(\"abstract\", r\"[\\r\\n]+\", \" \"))) \\\n",
    "             .withColumn(\"title\", regexp_replace(\"title\", r\"\\s+\", \" \")) \\\n",
    "             .withColumn(\"abstract\", regexp_replace(\"abstract\", r\"\\s+\", \" \"))\n",
    "\n",
    "# Optionally, make text lowercase (for consistent analysis, though Elasticsearch will handle casing)\n",
    "df_clean = df_clean.withColumn(\"title\", lower(\"title\")) \\\n",
    "                   .withColumn(\"abstract\", lower(\"abstract\")) \\\n",
    "                   .withColumn(\"authors\", lower(\"authors\"))\n",
    "\n",
    "# Normalize authors: replace ' and ' with comma, remove trailing ' and'\n",
    "df_clean = df_clean.withColumn(\"authors\", regexp_replace(\"authors\", r\"\\sand\\s\", \", \")) \\\n",
    "                   .withColumn(\"authors\", regexp_replace(\"authors\", r\"\\s+\", \" \"))\n",
    "\n",
    "# Split categories into array of category codes\n",
    "df_clean = df_clean.withColumn(\"categories\", split(\"categories\", \" \"))\n",
    "\n",
    "\n",
    "df_clean.printSchema()\n",
    "print(\"Total records:\", df.count())\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514235e0-8624-4691-b1ca-219a05c38817",
   "metadata": {},
   "source": [
    "The above transformations:\n",
    "- Use regexp_replace to replace newlines (\\r\\n) with spaces and condense multiple spaces to one.\n",
    "- Lowercase the text in title, abstract, authors.\n",
    "- In authors, attempt to replace the word \" and \" with a comma+space, so that authors like \"Smith and Doe\" become \"Smith, Doe\", then all multiple spaces condensed. This way authors are separated uniformly by commas.\n",
    "- Split the categories string on spaces into an array (e.g., \"cs.AI cs.CL\" becomes [\"cs.AI\",\"cs.CL\"])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc8ad0e-9ff9-44d3-99aa-75e6ba10ec64",
   "metadata": {},
   "source": [
    "## Text Processing and Feature Extraction\n",
    "\n",
    "#### Tokenization and Stopword Removal\n",
    "\n",
    "We will tokenize the abstract (and optionally title) text into words, and then remove common stop words (like \"the\", \"and\", \"of\", etc.) which are not useful in search queries. PySpark's ML library provides Tokenizer/RegexTokenizer and StopWordsRemover for this purpose. Let's tokenize the abstracts into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "689a07f4-b65a-4ae0-82e5-13f44bb70272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|                                                                        abstract|                                                                  filtered_words|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|a fully differential calculation in perturbative quantum chromodynamics is pr...|[fully, differential, calculation, perturbative, quantum, chromodynamics, pre...|\n",
      "|we describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use ...|[describe, new, algorithm, k, ell, pebble, game, colors, use, obtain, charact...|\n",
      "|the evolution of earth-moon system is described by the dark matter field flui...|[evolution, earth, moon, system, described, dark, matter, field, fluid, model...|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "# Tokenizer to split text on non-word characters (this will break text into words)\n",
    "tokenizer = RegexTokenizer(inputCol=\"abstract\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "words_data = tokenizer.transform(df_clean)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_data = remover.transform(words_data)\n",
    "\n",
    "filtered_data.select(\"abstract\", \"filtered_words\").show(3, truncate=80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a40a89-845a-4047-87b0-b658d711d793",
   "metadata": {},
   "source": [
    "#### Computing TF-IDF Features\n",
    "Next, we compute TF-IDF (Term Frequency–Inverse Document Frequency) vectors for the documents. TF-IDF is a numerical statistic that reflects how important a word is to a document in a corpus​. \n",
    "\n",
    "We will use Spark's HashingTF to hash words into term frequency vectors, then IDF to scale them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8269c8e-1c92-4905-b491-15b0c01a3753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------------------------------------------------------------+\n",
      "|       id|                                                                        features|\n",
      "+---------+--------------------------------------------------------------------------------+\n",
      "|0704.0001|(10000,[134,157,282,436,717,944,1072,1080,1113,1161,1226,1253,1439,1481,1695,...|\n",
      "|0704.0002|(10000,[221,274,310,521,585,625,705,870,885,1055,1296,1468,1541,2093,2241,225...|\n",
      "|0704.0003|(10000,[20,157,253,258,316,327,399,617,697,735,922,1016,1038,1622,1695,1896,1...|\n",
      "+---------+--------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# HashingTF to map the filtered words to a fixed-length feature vector\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "featurized_data = hashingTF.transform(filtered_data)\n",
    "# Compute the IDF model on the corpus\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurized_data)\n",
    "rescaled_data = idfModel.transform(featurized_data)\n",
    "\n",
    "# Check the TF-IDF feature vector for a sample\n",
    "rescaled_data.select(\"id\", \"features\").show(3, truncate=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c553075-55fd-48f0-832e-c6ebc0fce08d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
