{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69df714-130e-48e0-9a6e-3baadd2698e2",
   "metadata": {},
   "source": [
    "# Test downloading data and checking its format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6d7ae0-a12e-4928-9346-e94eb8a6e797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 entries.\n",
      "Hierarchical Reinforcement Learning with the MAXQ Value Function\n",
      "  Decomposition\n",
      "http://arxiv.org/abs/cs/9905014v1 1999-05-21T14:26:07Z\n",
      "{'id': 'http://arxiv.org/abs/cs/9905014v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/cs/9905014v1', 'updated': '1999-05-21T14:26:07Z', 'updated_parsed': time.struct_time(tm_year=1999, tm_mon=5, tm_mday=21, tm_hour=14, tm_min=26, tm_sec=7, tm_wday=4, tm_yday=141, tm_isdst=0), 'published': '1999-05-21T14:26:07Z', 'published_parsed': time.struct_time(tm_year=1999, tm_mon=5, tm_mday=21, tm_hour=14, tm_min=26, tm_sec=7, tm_wday=4, tm_yday=141, tm_isdst=0), 'title': 'Hierarchical Reinforcement Learning with the MAXQ Value Function\\n  Decomposition', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=cat:cs.LG&start=0&max_results=100', 'value': 'Hierarchical Reinforcement Learning with the MAXQ Value Function\\n  Decomposition'}, 'summary': 'This paper presents the MAXQ approach to hierarchical reinforcement learning\\nbased on decomposing the target Markov decision process (MDP) into a hierarchy\\nof smaller MDPs and decomposing the value function of the target MDP into an\\nadditive combination of the value functions of the smaller MDPs. The paper\\ndefines the MAXQ hierarchy, proves formal results on its representational\\npower, and establishes five conditions for the safe use of state abstractions.\\nThe paper presents an online model-free learning algorithm, MAXQ-Q, and proves\\nthat it converges wih probability 1 to a kind of locally-optimal policy known\\nas a recursively optimal policy, even in the presence of the five kinds of\\nstate abstraction. The paper evaluates the MAXQ representation and MAXQ-Q\\nthrough a series of experiments in three domains and shows experimentally that\\nMAXQ-Q (with state abstractions) converges to a recursively optimal policy much\\nfaster than flat Q learning. The fact that MAXQ learns a representation of the\\nvalue function has an important benefit: it makes it possible to compute and\\nexecute an improved, non-hierarchical policy via a procedure similar to the\\npolicy improvement step of policy iteration. The paper demonstrates the\\neffectiveness of this non-hierarchical execution experimentally. Finally, the\\npaper concludes with a comparison to related work and a discussion of the\\ndesign tradeoffs in hierarchical reinforcement learning.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=cat:cs.LG&start=0&max_results=100', 'value': 'This paper presents the MAXQ approach to hierarchical reinforcement learning\\nbased on decomposing the target Markov decision process (MDP) into a hierarchy\\nof smaller MDPs and decomposing the value function of the target MDP into an\\nadditive combination of the value functions of the smaller MDPs. The paper\\ndefines the MAXQ hierarchy, proves formal results on its representational\\npower, and establishes five conditions for the safe use of state abstractions.\\nThe paper presents an online model-free learning algorithm, MAXQ-Q, and proves\\nthat it converges wih probability 1 to a kind of locally-optimal policy known\\nas a recursively optimal policy, even in the presence of the five kinds of\\nstate abstraction. The paper evaluates the MAXQ representation and MAXQ-Q\\nthrough a series of experiments in three domains and shows experimentally that\\nMAXQ-Q (with state abstractions) converges to a recursively optimal policy much\\nfaster than flat Q learning. The fact that MAXQ learns a representation of the\\nvalue function has an important benefit: it makes it possible to compute and\\nexecute an improved, non-hierarchical policy via a procedure similar to the\\npolicy improvement step of policy iteration. The paper demonstrates the\\neffectiveness of this non-hierarchical execution experimentally. Finally, the\\npaper concludes with a comparison to related work and a discussion of the\\ndesign tradeoffs in hierarchical reinforcement learning.'}, 'authors': [{'name': 'Thomas G. Dietterich'}], 'author_detail': {'name': 'Thomas G. Dietterich'}, 'author': 'Thomas G. Dietterich', 'arxiv_comment': '63 pages, 15 figures', 'links': [{'href': 'http://arxiv.org/abs/cs/9905014v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/cs/9905014v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.2.6', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "State Abstraction in MAXQ Hierarchical Reinforcement Learning\n",
      "http://arxiv.org/abs/cs/9905015v1 1999-05-21T14:49:39Z\n",
      "{'id': 'http://arxiv.org/abs/cs/9905015v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/cs/9905015v1', 'updated': '1999-05-21T14:49:39Z', 'updated_parsed': time.struct_time(tm_year=1999, tm_mon=5, tm_mday=21, tm_hour=14, tm_min=49, tm_sec=39, tm_wday=4, tm_yday=141, tm_isdst=0), 'published': '1999-05-21T14:49:39Z', 'published_parsed': time.struct_time(tm_year=1999, tm_mon=5, tm_mday=21, tm_hour=14, tm_min=49, tm_sec=39, tm_wday=4, tm_yday=141, tm_isdst=0), 'title': 'State Abstraction in MAXQ Hierarchical Reinforcement Learning', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=cat:cs.LG&start=0&max_results=100', 'value': 'State Abstraction in MAXQ Hierarchical Reinforcement Learning'}, 'summary': 'Many researchers have explored methods for hierarchical reinforcement\\nlearning (RL) with temporal abstractions, in which abstract actions are defined\\nthat can perform many primitive actions before terminating. However, little is\\nknown about learning with state abstractions, in which aspects of the state\\nspace are ignored. In previous work, we developed the MAXQ method for\\nhierarchical RL. In this paper, we define five conditions under which state\\nabstraction can be combined with the MAXQ value function decomposition. We\\nprove that the MAXQ-Q learning algorithm converges under these conditions and\\nshow experimentally that state abstraction is important for the successful\\napplication of MAXQ-Q learning.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=cat:cs.LG&start=0&max_results=100', 'value': 'Many researchers have explored methods for hierarchical reinforcement\\nlearning (RL) with temporal abstractions, in which abstract actions are defined\\nthat can perform many primitive actions before terminating. However, little is\\nknown about learning with state abstractions, in which aspects of the state\\nspace are ignored. In previous work, we developed the MAXQ method for\\nhierarchical RL. In this paper, we define five conditions under which state\\nabstraction can be combined with the MAXQ value function decomposition. We\\nprove that the MAXQ-Q learning algorithm converges under these conditions and\\nshow experimentally that state abstraction is important for the successful\\napplication of MAXQ-Q learning.'}, 'authors': [{'name': 'Thomas G. Dietterich'}], 'author_detail': {'name': 'Thomas G. Dietterich'}, 'author': 'Thomas G. Dietterich', 'arxiv_comment': '7 pages, 2 figures', 'links': [{'href': 'http://arxiv.org/abs/cs/9905015v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/cs/9905015v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.2.6', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "Multiplicative Algorithm for Orthgonal Groups and Independent Component\n",
      "  Analysis\n",
      "http://arxiv.org/abs/cs/0001004v1 2000-01-07T06:20:53Z\n",
      "{'id': 'http://arxiv.org/abs/cs/0001004v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/cs/0001004v1', 'updated': '2000-01-07T06:20:53Z', 'updated_parsed': time.struct_time(tm_year=2000, tm_mon=1, tm_mday=7, tm_hour=6, tm_min=20, tm_sec=53, tm_wday=4, tm_yday=7, tm_isdst=0), 'published': '2000-01-07T06:20:53Z', 'published_parsed': time.struct_time(tm_year=2000, tm_mon=1, tm_mday=7, tm_hour=6, tm_min=20, tm_sec=53, tm_wday=4, tm_yday=7, tm_isdst=0), 'title': 'Multiplicative Algorithm for Orthgonal Groups and Independent Component\\n  Analysis', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=cat:cs.LG&start=0&max_results=100', 'value': 'Multiplicative Algorithm for Orthgonal Groups and Independent Component\\n  Analysis'}, 'summary': 'The multiplicative Newton-like method developed by the author et al. is\\nextended to the situation where the dynamics is restricted to the orthogonal\\ngroup. A general framework is constructed without specifying the cost function.\\nThough the restriction to the orthogonal groups makes the problem somewhat\\ncomplicated, an explicit expression for the amount of individual jumps is\\nobtained. This algorithm is exactly second-order-convergent. The global\\ninstability inherent in the Newton method is remedied by a\\nLevenberg-Marquardt-type variation. The method thus constructed can readily be\\napplied to the independent component analysis. Its remarkable performance is\\nillustrated by a numerical simulation.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=cat:cs.LG&start=0&max_results=100', 'value': 'The multiplicative Newton-like method developed by the author et al. is\\nextended to the situation where the dynamics is restricted to the orthogonal\\ngroup. A general framework is constructed without specifying the cost function.\\nThough the restriction to the orthogonal groups makes the problem somewhat\\ncomplicated, an explicit expression for the amount of individual jumps is\\nobtained. This algorithm is exactly second-order-convergent. The global\\ninstability inherent in the Newton method is remedied by a\\nLevenberg-Marquardt-type variation. The method thus constructed can readily be\\napplied to the independent component analysis. Its remarkable performance is\\nillustrated by a numerical simulation.'}, 'authors': [{'name': 'Toshinao Akuzawa'}], 'author_detail': {'name': 'Toshinao Akuzawa'}, 'arxiv_affiliation': 'RIKEN BSI', 'author': 'Toshinao Akuzawa', 'arxiv_comment': '11 pages, 2 figures', 'links': [{'href': 'http://arxiv.org/abs/cs/0001004v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/cs/0001004v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'G.1.6', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "base_url = \"http://export.arxiv.org/api/query?\"\n",
    "query = \"search_query=cat:cs.LG&start=0&max_results=100\"\n",
    "feed = feedparser.parse(base_url + query)\n",
    "print(f\"Found {len(feed.entries)} entries.\")\n",
    "for entry in feed.entries[:3]:\n",
    "    print(entry.title)\n",
    "    print(entry.id, entry.published)\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b9b87-2c5f-4aec-b4ab-8e6bb5814f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
