{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a56dc781-0a26-46a3-8b5b-658ab677d428",
   "metadata": {},
   "source": [
    "# PySpark Example Notebook\n",
    "\n",
    "This notebook demonstrates how to use Apache Spark with Python (PySpark) in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7929c0-3244-4aa9-b4f6-9a458be48236",
   "metadata": {},
   "source": [
    "## Checking Versions\n",
    "\n",
    "First, let's check which versions of Python and Spark we're using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b84d5ef-4475-464c-8443-a0ceebc728c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:40:35) [GCC 12.3.0]\n",
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Print Python and Spark versions\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession first\n",
    "spark = SparkSession.builder.appName(\"VersionCheck\").getOrCreate()\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed49d04f-2676-4942-8fda-c7c756e7cab0",
   "metadata": {},
   "source": [
    "## Creating a DataFrame\n",
    "\n",
    "Let's create a simple DataFrame with some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "392cc0e7-71ac-44e7-a9d9-ce7615ae0b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1| John| 25|\n",
      "|  2|Alice| 30|\n",
      "|  3|  Bob| 35|\n",
      "|  4|Sarah| 28|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a simple DataFrame\n",
    "data = [\n",
    "    (1, \"John\", 25),\n",
    "    (2, \"Alice\", 30),\n",
    "    (3, \"Bob\", 35),\n",
    "    (4, \"Sarah\", 28)\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e4230-307f-494c-8bf0-605dca6cd28b",
   "metadata": {},
   "source": [
    "## Transforming Data\n",
    "\n",
    "Now let's perform some transformations on our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2109374-37f6-466c-ae36-8d7d03d090f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  2|Alice| 30|\n",
      "|  3|  Bob| 35|\n",
      "|  4|Sarah| 28|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for people older than 25\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "older_than_25 = df.filter(col(\"age\") > 25)\n",
    "older_than_25.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fdfedb-c0d3-44a0-bfaf-f8ad377c55a0",
   "metadata": {},
   "source": [
    "## Using SQL with Spark\n",
    "\n",
    "Spark allows you to use SQL queries on your DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f08ad4-715a-4f0c-ac7a-7621aa6ccf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  4|Sarah| 28|\n",
      "|  2|Alice| 30|\n",
      "|  3|  Bob| 35|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register as a temporary view to use SQL\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Run SQL query\n",
    "sql_result = spark.sql(\"SELECT * FROM people WHERE age > 25 ORDER BY age\")\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39371f3-7223-4917-a61a-7cfdb9bc3e49",
   "metadata": {},
   "source": [
    "## Working with RDDs\n",
    "\n",
    "While DataFrames are the modern API, you can also work with RDDs (Resilient Distributed Datasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2acfdceb-13f0-421a-933b-bf9622f003ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of doubled even numbers from 1 to 100: 5100\n"
     ]
    }
   ],
   "source": [
    "# Create a simple RDD\n",
    "rdd = spark.sparkContext.parallelize(range(1, 101))\n",
    "\n",
    "# Perform some transformations\n",
    "result = rdd.filter(lambda x: x % 2 == 0).map(lambda x: x * 2).reduce(lambda x, y: x + y)\n",
    "print(f\"Sum of doubled even numbers from 1 to 100: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5929fc5-b598-41a5-854c-2d77518baa65",
   "metadata": {},
   "source": [
    "## More Complex Example: Word Count\n",
    "\n",
    "Let's implement the classic word count example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c2d7bb-1337-4c3b-a7e1-dc0f21565bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark: 4\n",
      "the: 3\n",
      "an: 3\n",
      "for: 3\n",
      "data: 3\n",
      "apache: 2\n",
      "implicit: 2\n",
      "and: 2\n",
      "provides: 2\n",
      "programming: 2\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"\"\"Apache Spark is an open-source unified analytics engine for large-scale data processing.\n",
    "Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
    "Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since. Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.\"\"\"\n",
    "\n",
    "# Create an RDD from the text\n",
    "import re\n",
    "text_rdd = spark.sparkContext.parallelize(text.split(\"\\n\"))\n",
    "\n",
    "# Split into words, convert to lowercase, remove punctuation, and count\n",
    "word_counts = text_rdd \\\n",
    "    .flatMap(lambda line: re.sub(r'[^a-zA-Z ]', '', line.lower()).split(\" \")) \\\n",
    "    .filter(lambda word: len(word) > 0) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words\n",
    "for word, count in word_counts.take(10):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d1c00b-85f2-4467-8fb6-312fee5a3c28",
   "metadata": {},
   "source": [
    "## Using DataFrame API for Word Count\n",
    "\n",
    "The same word count can be implemented more elegantly using the DataFrame API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cac5175-459c-4883-a893-aa13a90491ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "|      spark|    4|\n",
      "|        for|    3|\n",
      "|       data|    3|\n",
      "|         an|    3|\n",
      "|        the|    3|\n",
      "|     apache|    2|\n",
      "|   provides|    2|\n",
      "|programming|    2|\n",
      "|       with|    2|\n",
      "|parallelism|    2|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the text\n",
    "from pyspark.sql.functions import explode, split, lower, regexp_replace, count\n",
    "\n",
    "# Split the text into lines\n",
    "lines_df = spark.createDataFrame(text.split(\"\\n\"), \"string\").toDF(\"line\")\n",
    "\n",
    "# Process the text and count words\n",
    "word_counts_df = lines_df \\\n",
    "    .select(explode(split(regexp_replace(lower(\"line\"), \"[^a-zA-Z ]\", \"\"), \" \")).alias(\"word\")) \\\n",
    "    .filter(\"length(word) > 0\") \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Show the top 10 words\n",
    "word_counts_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5785b42c-ae9e-49d9-b02b-8c0c9e51cfad",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored:\n",
    "\n",
    "1. Creating and manipulating DataFrames with PySpark\n",
    "2. Using SQL queries with Spark\n",
    "3. Working with RDDs in Python\n",
    "4. Implementing a word count algorithm using both RDD and DataFrame APIs\n",
    "\n",
    "PySpark provides a Python-friendly interface to Spark's powerful distributed computing capabilities, making it accessible to data scientists and engineers who are familiar with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c46304-e493-4798-adb9-8ace623b7732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
